{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae2ff0-d228-42bb-98be-5be6722131fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ashery/chexpert\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be929ff-2f43-4f76-9ea9-ad564ae09a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split  # new import\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from timm import create_model\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5252cdd-d1bf-4f2a-aded-3c603315a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuration ===\n",
    "DATA_DIR = \"F:\\\\CheXpert\\\\archive\"\n",
    "TRAIN_CSV = os.path.join(DATA_DIR, \"train.csv\")\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-4\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248c32ba-a2dd-4618-9de2-58a03c1ca2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Full 14 CheXpert labels ===\n",
    "label_columns = [\n",
    "    \"No Finding\", \"Enlarged Cardiomediastinum\", \"Cardiomegaly\", \"Lung Opacity\",\n",
    "    \"Lung Lesion\", \"Edema\", \"Consolidation\", \"Pneumonia\", \"Atelectasis\",\n",
    "    \"Pneumothorax\", \"Pleural Effusion\", \"Pleural Other\", \"Fracture\", \"Support Devices\"\n",
    "]\n",
    "NUM_CLASSES = len(label_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e87ddf-0144-4fc4-a016-3cef5b5fd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Data Preparation with Train/Test Split ===\n",
    "def load_and_split(csv_path, test_size=0.15, random_state=42):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['Path'] = df['Path'].apply(\n",
    "        lambda x: os.path.normpath(f\"{DATA_DIR}/{x.replace('CheXpert-v1.0-small/', '')}\")\n",
    "    )\n",
    "    df[label_columns] = df[label_columns].fillna(0).astype(int)\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    return train_df.reset_index(drop=True), valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce632db-dc02-4219-93dd-8901b048c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Custom Dataset ===\n",
    "class CheXpertDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image = Image.open(row['Path']).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        labels = row[label_columns].to_numpy(dtype=np.float32)\n",
    "        return image, torch.from_numpy(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9beb13-97ad-47d5-a44b-186527241c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Transforms ===\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])\n",
    "transform_valid = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e493c60-cb3e-44b5-a975-7325a3ea6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loss Function ignoring -1 labels ===\n",
    "def masked_bce_with_logits_loss(logits, targets):\n",
    "    mask = targets != -1\n",
    "    logits_m = logits[mask]\n",
    "    targets_m = targets[mask]\n",
    "    return F.binary_cross_entropy_with_logits(logits_m, targets_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da27396b-69fa-4148-8ff8-08f206735ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Training Function with Accuracy ===\n",
    "def train_model(model, train_loader, valid_loader, optimizer, scheduler, device, epochs=NUM_EPOCHS):\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accs, valid_accs = [], []\n",
    "    tokens_per_sec_list = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with autocast():\n",
    "                logits = model(images)\n",
    "                loss = masked_bce_with_logits_loss(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            mask = labels != -1\n",
    "            correct += (preds[mask] == labels[mask]).sum().item()\n",
    "            total += mask.sum().item()\n",
    "\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        acc = 100 * correct / total if total > 0 else 0\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accs.append(acc)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        start = time.time()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                with autocast():\n",
    "                    logits = model(images)\n",
    "                    loss = masked_bce_with_logits_loss(logits, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "                mask = labels != -1\n",
    "                val_correct += (preds[mask] == labels[mask]).sum().item()\n",
    "                val_total += mask.sum().item()\n",
    "        elapsed = time.time() - start\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        val_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
    "        valid_losses.append(avg_val_loss)\n",
    "        valid_accs.append(val_acc)\n",
    "        tokens_per_sec_list.append(len(valid_loader.dataset) / elapsed)\n",
    "\n",
    "        print(f\"Train Loss: {avg_loss:.4f}, Train Acc: {acc:.2f}%\")\n",
    "        print(f\"Valid Loss: {avg_val_loss:.4f}, Valid Acc: {val_acc:.2f}%, Speed: {tokens_per_sec_list[-1]:.2f} img/s\")\n",
    "\n",
    "    # Plot Loss and Accuracy\n",
    "    epochs_range = range(1, epochs+1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs_range, train_losses, 'o-', label='Train Loss')\n",
    "    plt.plot(epochs_range, valid_losses, 's-', label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Loss Curve')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs_range, train_accs, 'o-', label='Train Acc')\n",
    "    plt.plot(epochs_range, valid_accs, 's-', label='Valid Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title('Accuracy Curve')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b7e16f-70f1-4194-86b9-1bfa68648afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and split dataset\n",
    "df_train, df_valid = load_and_split(TRAIN_CSV, test_size=0.15)\n",
    "\n",
    "train_dataset = CheXpertDataset(df_train, transform=transform)\n",
    "valid_dataset = CheXpertDataset(df_valid, transform=transform_valid)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model = create_model('swin_base_patch4_window7_224', pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "train_model(model, train_loader, valid_loader, optimizer, scheduler, DEVICE)\n",
    "\n",
    "save_path = \"D:\\\\projects\\\\Pycharm_python1\\\\CheXpert\\\\Weights\\\\chexpert_swin_14class.pth\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "torch.save(model.cpu().state_dict(), save_path)\n",
    "print(f\"âœ… Model saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
