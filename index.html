<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Transformer for Medical AI</title>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      margin: 2rem;
      line-height: 1.6;
    }
    img {
      display: block;
      margin: 1.5rem auto;
      max-width: 90%;
    }
    h1 {
      text-align: center;
      font-size: 28px;
      font-weight: bold;
    }
    .figure {
      text-align: center;
      margin-bottom: 1rem;
    }
     .caption {
      font-size: 16px;
      text-align: justify;
      color: #333;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
    .caption b {
      font-weight: bold;
    }
    .abstract-title {
      text-align: center;
      font-size: 24px;
      font-weight: bold;
      margin-top: 2rem;
    }
    .abstract-body {
      font-size: 20px;
      text-align: justify;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
  </style>
</head>
<body>

<h1>Transformer for Medical AI</h1>

<div class="figure">
  <img src="./main.png" alt="Main Image">
</div>

<div class="abstract-title">Abstract</div>
<div class="abstract-body">
  We utilized the CheXpert dataset, a large-scale chest X-ray dataset, to diagnose chest conditions based on 14 labels and to localize pathological regions using heatmaps. To achieve this, we applied three vision transformer models—ViT, BEiT, and Swin. Initial experiments conducted on a test subset showed that the ViT Transformer model achieved the highest accuracy. Based on this result, we employed the ViT model on the full dataset to further improve diagnostic performance and enhance the precision of heatmap-based localization.
</div>
  
<div class="figure">
  <img src="./figure1.png" alt="Figure 1">
  <div class="caption">
    <b>Figure 1:</b> presents the performance of three vision transformer models—ViT, BEiT, and Swin—evaluated using a subset of the test dataset. Although the Swin model demonstrated high accuracy during training, it showed poor and unstable performance on the test set. The BEiT model achieved higher test accuracy than the ViT model; however, the ViT model exhibited more consistent improvement in accuracy as the number of epochs increased and showed better performance during training. Based on this trend, we hypothesized that the ViT model would achieve further improvement when trained on the full dataset. Therefore, we selected the ViT model for our final experiments.
  </div>
</div>

<h2>Figure 2</h2>
<div class="figure">
  <img src="./figure2.png" alt="Figure 2">
  <div class="caption">
    <b>Figure 2:</b> shows the results of training and testing the ViT model using the full dataset, based on the findings from Figure 1. However, no significant improvement was observed in performance.
  </div>
</div>

<h2>Figure 3</h2>
<div class="figure">
  <img src="./figure3.png" alt="Figure 3">
  <div class="caption">
    <b>Figure 3:</b> The CLS token, which is trained to aggregate global information from the entire image, can be used to generate 2D heatmaps by visualizing the attention scores assigned to each patch. Figure 3 illustrates how the attention maps evolve as the model improves: the earlier model fails to focus accurately on the lesion, while the updated model correctly identifies disease-relevant regions.
  </div>
</div>

</body>
</html>
