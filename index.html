<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Transformer for Medical AI</title>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      margin: 2rem;
      line-height: 1.6;
    }
    img {
      display: block;
      margin: 2em auto 0 auto;
      max-width: 60%;
    }
    h1 {
      text-align: center;
      font-size: 40px;
      font-weight: bold;
    }
    h2 {
      text-align: center;
      font-size: 20px;
      font-weight: bold;
    }
    h3 {
      text-align: center;
      font-size: 24px;
      font-weight: bold;
    }
    .author-list h2 {
      line-height: 1.2;
      margin: 0.2rem 0;
    }
    .figure {
      text-align: center;
      margin-bottom: 1rem;
    }
     .caption {
      font-size: 16px;
      text-align: justify;
      color: #333;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
    .caption b {
      font-weight: bold;
    }
    .abstract-title {
      text-align: center;
      font-size: 24px;
      font-weight: bold;
      margin-top: 2rem;
    }
    .abstract-body {
      font-size: 16px;
      text-align: justify;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
  </style>
</head>
<body>

<h1>Transformers for Medical AI</h1>

<h3>Project 6<h3>

<div class="author-list">
  <h2>Zheng Hexing 2023311430</h2>
  <h2>Chang Hwan Kim 2024321234</h2>
  <h2>Maftuna Ziyamova 2024311551</h2>
  <h2>Lee Woo Bin 2025311560</h2>
</div>

<div class="figure">
  <img src="./Assets/main1.png" alt="Main Image">
</div>

<div class="abstract-title">Abstract</div>
<div class="abstract-body">
  We utilized the CheXpert dataset, a large-scale chest X-ray dataset, to diagnose chest conditions based on 14 labels and to localize pathological regions using heatmaps. To achieve this, we applied three vision transformer modelsâ€”ViT, BEiT, and Swin. Initial experiments conducted on a test subset showed that the ViT Transformer model achieved the highest accuracy. Based on this result, we employed the ViT model on the full dataset to further improve diagnostic performance and enhance the precision of heatmap-based localization.
</div>

<div class="abstract-title">Dataset</div>
<div class="abstract-body">
  We used the CheXpert-v1.0-small dataset, a downsized version of the original CheXpert dataset. It contains 224,316 chest radiographs from 65,240 patients, labeled with 14 clinical observations. The labels were generated by an automated labeling system capable of detecting and classifying findings, including cases with inherent uncertainty. A validation set of 200 studies was manually annotated by three board-certified radiologists to ensure reliability.
</div>
<div style="text-align: center; margin: 0 auto 4rem auto;">
  <a href="https://www.kaggle.com/datasets/ashery/chexpert" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333; margin: 0 0.5rem;">
    CheXpert Dataset
  </a>
</div>

<div class="abstract-title" style="margin-top: 4rem;">Experiment <span style="font-weight: normal; font-size: 20px;">(Three Vision Transformers)</span></div>

<style>
  .tab-container {
    display: flex;
    justify-content: center;
    margin: 1rem 0;
    gap: 1rem;
  }
  .tab-button {
    padding: 0.7rem 2rem;
    font-size: 16px;
    border: 2px solid #aaa;
    background-color: white;
    cursor: pointer;
    border-radius: 6px;
    transition: background-color 0.3s;
    font-family: 'Noto Sans', sans-serif;
    width: 160px;
    text-align: center;
  }
  .tab-button:hover {
    background-color: #eee;
  }
  .tab-button.active {
    background-color: #ddd;
    font-weight: bold;
  }
  .tab-content {
    display: none;
    max-width: 800px;
    margin: 1rem auto;
    text-align: justify;
    font-size: 16px;
  }
  .tab-content.active {
    display: block;
  }
</style>

<div class="tab-container">
  <button class="tab-button active" onclick="showTab('vit')">ViT</button>
  <button class="tab-button" onclick="showTab('beit')">BEiT</button>
  <button class="tab-button" onclick="showTab('swin')">Swin</button>
</div>

<div id="vit" class="tab-content active" style="margin-bottom: 4rem;">
  The Vision Transformer (ViT) applies the standard Transformer architecture directly to image patches, treating them as sequences similar to words in natural language. It splits the input image into fixed-size patches and processes them with self-attention mechanisms. ViT is known for its simplicity and scalability, performing well with large datasets but requiring significant data and compute resources to outperform CNN-based models.
  <div style="text-align: center; margin-top: 1rem;">
    <img src="./Assets/ViT.png" alt="ViT Image" style="max-width: 75%; margin-top: 1rem;">
    <div style="margin-top: 1rem;">
      <a href="./Assets/vit_transformer.ipynb" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333;">ViT Code(Download)</a>
    </div>
  </div>
</div>

<div id="beit" class="tab-content" style="margin-bottom: 4rem;">
  BEiT builds upon ViT by introducing a pretraining strategy similar to BERT in NLP. It treats image patches as discrete visual tokens and learns bidirectional representations using a masked image modeling objective. This enables the model to better capture contextual relationships within the image, significantly improving performance in downstream tasks, especially when labeled data is limited.
  <div style="text-align: center; margin-top: 1rem;">
    <img src="./Assets/BEiT.png" alt="BEiT Image" style="max-width: 75%; margin-top: 1rem;">
    <div style="margin-top: 1rem;">
      <a href="./Assets/BEiT_transformer.ipynb" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333;">BEiT Code(Download)</a>
    </div>
  </div>
</div>

<div id="swin" class="tab-content" style="margin-bottom: 4rem;">
  The Swin Transformer introduces a hierarchical architecture that processes images through non-overlapping local windows with shifted configurations across layers. This design enables both local and global representation learning while maintaining computational efficiency. Its ability to model long-range dependencies and multi-scale features makes it particularly effective for dense prediction tasks such as detection and segmentation.
  <div style="text-align: center; margin-top: 1rem;">
    <img src="./Assets/Swin.png" alt="Swin Image" style="max-width: 75%; margin-top: 1rem;">
    <div style="margin-top: 1rem;">
      <a href="./Assets/Swin_transformer.ipynb" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333;">Swin Code(Download)</a>
    </div>
  </div>
</div>

<script>
  function showTab(id) {
    const tabs = document.querySelectorAll('.tab-button');
    const contents = document.querySelectorAll('.tab-content');
    tabs.forEach(tab => tab.classList.remove('active'));
    contents.forEach(content => content.classList.remove('active'));
    document.querySelector(`#${id}`).classList.add('active');
    event.target.classList.add('active');
  }
</script>
  
<div class="figure">
  <img src="./Assets/figure1.png" alt="Figure 1">
  <div class="caption">
    <b>Figure 1 :</b> This slide presents a comparative evaluation of three transformer-based architectures: Vision Transformer (ViT), Swin Transformer, and BEiT Transformer. Their performance was measured across 10 training epochs using both accuracy and loss metrics on training and test datasets. The accuracy plot on the left demonstrates that although the Swin Transformer achieves the highest accuracy during training, the BEiT Transformer consistently outperforms the others on the test set. This suggests that BEiT has superior generalization capabilities to unseen data. Correspondingly, the loss plot on the right reinforces this observation. The BEiT Transformer maintains the lowest test loss throughout all epochs, indicating its effectiveness in minimizing prediction error on novel inputs. In contrast, both ViT and Swin exhibit either increasing or plateauing test loss trends, which may be indicative of overfitting. Overall, among the evaluated models, the BEiT Transformer demonstrates the most favorable trade-off between training performance and generalization, establishing it as the most robust architecture in this comparison.
  </div>
</div>

<div class="figure">
  <img src="./Assets/figure2.png" alt="Figure 2">
  <div class="caption" style="margin-bottom: 3rem;">
    <b>Figure 2 :</b> This slide presents a performance comparison of the Vision Transformer (ViT) model trained on the full CheXpert dataset versus its downsampled counterpart. As illustrated in the graphs, both accuracy and loss metrics remain nearly identical across training epochs. This outcome suggests that the ViT model maintains comparable performance even when trained on a reduced dataset, indicating a degree of robustness. Such results highlight the potential for significant computational efficiency without compromising diagnostic effectiveness.
  </div>
</div>

<div class="figure">
  <img src="./Assets/figure3.1.png" alt="Figure 3">
  <div class="caption">
    <b>Figure 3 :</b> The CLS token, which is trained to aggregate global information from the entire image, can be used to generate 2D heatmaps by visualizing the attention scores assigned to each patch. Figure 3 illustrates how the attention maps evolve as the model improves: the earlier model fails to focus accurately on the lesion, while the updated model correctly identifies disease-relevant regions.
  </div>
</div>
  
<div style="text-align: center; margin-top: 4rem;">
  <a href="./Assets/Transformer for Medical AI.pdf" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333; margin: 0 0.5rem;">
    Final Presentation Report
  </a>
</div>

</body>
</html>
