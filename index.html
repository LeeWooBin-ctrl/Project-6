<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Transformer for Medical AI</title>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      margin: 2rem;
      line-height: 1.6;
    }
    img {
      display: block;
      margin: 1.5rem auto 0 auto;
      max-width: 65%;
    }
    h1 {
      text-align: center;
      font-size: 40px;
      font-weight: bold;
    }
    h2 {
      text-align: center;
      font-size: 20px;
      font-weight: bold;
    }
    h3 {
      text-align: center;
      font-size: 24px;
      font-weight: bold;
    }
    .author-list h2 {
      line-height: 1.2;
      margin: 0.2rem 0;
    }
    .figure {
      text-align: center;
      margin-bottom: 1rem;
    }
     .caption {
      font-size: 16px;
      text-align: justify;
      color: #333;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
    .caption b {
      font-weight: bold;
    }
    .abstract-title {
      text-align: center;
      font-size: 24px;
      font-weight: bold;
      margin-top: 2rem;
    }
    .abstract-body {
      font-size: 16px;
      text-align: justify;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
  </style>
</head>
<body>

<h1>Transformer for Medical AI</h1>

<h3>Project 6<h3>

<div class="author-list">
  <h2>Zheng Hexing 2023311430</h2>
  <h2>Chang Hwan Kim 2024321234</h2>
  <h2>Maftuna Ziyamova 2024311551</h2>
  <h2>Lee Woo Bin 2025311560</h2>
</div>

<div class="figure">
  <img src="./main.png" alt="Main Image">
</div>

<div class="abstract-title">Abstract</div>
<div class="abstract-body">
  We utilized the CheXpert dataset, a large-scale chest X-ray dataset, to diagnose chest conditions based on 14 labels and to localize pathological regions using heatmaps. To achieve this, we applied three vision transformer modelsâ€”ViT, BEiT, and Swin. Initial experiments conducted on a test subset showed that the ViT Transformer model achieved the highest accuracy. Based on this result, we employed the ViT model on the full dataset to further improve diagnostic performance and enhance the precision of heatmap-based localization.
</div>

<div class="abstract-title">Dastaset</div>
<div class="abstract-body">
  We used the CheXpert-v1.0-small dataset, a downsized version of the original CheXpert dataset. It contains 224,316 chest radiographs from 65,240 patients, labeled with 14 clinical observations. The labels were generated by an automated labeling system capable of detecting and classifying findings, including cases with inherent uncertainty. A validation set of 200 studies was manually annotated by three board-certified radiologists to ensure reliability.
</div>
<div class="caption" style="text-align: center; font-size: 16px; margin-top: 1rem;">
  ðŸ”— <a href="https://www.kaggle.com/datasets/ashery/chexpert" target="_blank">CheXpert Dataset</a>
</div>
  
<div class="figure">
  <img src="./figure1.png" alt="Figure 1">
  <div class="caption">
    <b>Figure 1 :</b> presents the performance of three vision transformer modelsâ€”ViT, BEiT, and Swinâ€”evaluated using a subset of the test dataset. Although the Swin model demonstrated high accuracy during training, it showed poor and unstable performance on the test set. The BEiT model achieved higher test accuracy than the ViT model; however, the ViT model exhibited more consistent improvement in accuracy as the number of epochs increased and showed better performance during training. Based on this trend, we hypothesized that the ViT model would achieve further improvement when trained on the full dataset. Therefore, we selected the ViT model for our final experiments.
  </div>
</div>

<div class="figure">
  <img src="./figure2.png" alt="Figure 2">
  <div class="caption">
    <b>Figure 2 :</b> shows the results of training and testing the ViT model using the full dataset, based on the findings from Figure 1. However, no significant improvement was observed in performance.
  </div>
</div>

<div class="figure">
  <img src="./figure3.png" alt="Figure 3">
  <div class="caption">
    <b>Figure 3 :</b> The CLS token, which is trained to aggregate global information from the entire image, can be used to generate 2D heatmaps by visualizing the attention scores assigned to each patch. Figure 3 illustrates how the attention maps evolve as the model improves: the earlier model fails to focus accurately on the lesion, while the updated model correctly identifies disease-relevant regions.
  </div>
</div>
  
<div class="caption" style="text-align: center; font-size: 16px; margin-top: 3rem;">
  ðŸ‘‰ <a href="./Transformers for Medical AI.pdf" target="_blank">Click here to view the Final Presentation report</a>
</div>

</body>
</html>
