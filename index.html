<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title>Transformer for Medical AI</title>
  <link href="https://fonts.googleapis.com/css2?family=Noto+Sans&display=swap" rel="stylesheet">
  <style>
    body {
      font-family: 'Noto Sans', sans-serif;
      margin: 2rem;
      line-height: 1.6;
    }
    img {
      display: block;
      margin: 2em auto 0 auto;
      max-width: 60%;
    }
    h1 {
      text-align: center;
      font-size: 40px;
      font-weight: bold;
    }
    h2 {
      text-align: center;
      font-size: 20px;
      font-weight: bold;
    }
    h3 {
      text-align: center;
      font-size: 24px;
      font-weight: bold;
    }
    .author-list h2 {
      line-height: 1.2;
      margin: 0.2rem 0;
    }
    .figure {
      text-align: center;
      margin-bottom: 1rem;
    }
     .caption {
      font-size: 16px;
      text-align: justify;
      color: #333;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
    .caption b {
      font-weight: bold;
    }
    .abstract-title {
      text-align: center;
      font-size: 24px;
      font-weight: bold;
      margin-top: 2rem;
    }
    .abstract-body {
      font-size: 16px;
      text-align: justify;
      max-width: 800px;
      margin: 0 auto 2rem auto;
    }
    .highlight {
      color: #1565c0;  
      font-weight: bold;
    }    
  </style>
</head>
<body>

<h1>Transformers for Medical AI</h1>

<h3>Project 6<h3>

<div class="author-list">
  <h2>Zheng Hexing 2023311430</h2>
  <h2>Chang Hwan Kim 2024321234</h2>
  <h2>Maftuna Ziyamova 2024311551</h2>
  <h2>Lee Woo Bin 2025311560</h2>
</div>

<div class="figure">
  <img src="./Assets/main1.png" alt="Main Image">
</div>

<div class="abstract-title">Abstract</div>
<div class="abstract-body">
  <p>
  We utilized the <b>CheXpert</b> dataset, a large-scale chest X-ray benchmark, to perform multi-label classification of 14 thoracic diseases and to localize pathological regions using heatmap-based explainability techniques.
</p>
<p>
  To accomplish this, we evaluated three vision transformer architectures—<b>ViT</b>, <b>BEiT</b>, and <b>Swin Transformer</b>.
</p>
<p>
  Preliminary experiments on a test subset indicated that <b>ViT</b> outperformed the others in terms of classification accuracy. Based on this finding, we employed the ViT model on the full dataset to enhance diagnostic performance and improve the spatial precision of the generated heatmaps.
</p>
</div>

<div class="abstract-title">Dataset</div>
<div class="abstract-body">
  <p>
  We used the <b>CheXpert-v1.0-small</b> dataset, a downsized version of the original <b>CheXpert</b> dataset. It contains <b>224,316 chest radiographs</b> from <b>65,240 patients</b>, labeled with <b>14 clinical observations</b>.
</p>

<p>
  The labels were generated using an <b>automated labeling system</b> capable of detecting and classifying findings, including those with <b>inherent uncertainty</b>.
</p>
<p>
  To ensure labeling reliability, a validation set of <strong>200 studies</strong> was <strong>manually annotated</strong> by three <strong>board-certified radiologists</strong>.
</p>
</div>
<div style="text-align: center; margin: 0 auto 4rem auto;">
<div style="text-align: center;">
  <a href="https://www.kaggle.com/datasets/ashery/chexpert" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333; margin: 0 0.5rem;">
    CheXpert Dataset
  </a>
  <a href="https://stanfordaimi.azurewebsites.net/datasets/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333; margin: 0 0.5rem;">
    CheXpert Full Dataset
  </a>
</div>
</div>

<div class="abstract-title" style="margin-top: 4rem;">BackGround <span style="font-weight: normal; font-size: 20px;">(Three Vision Transformers)</span></div>

<style>
  .tab-container {
    display: flex;
    justify-content: center;
    margin: 1rem 0;
    gap: 1rem;
  }
  .tab-button {
    padding: 0.7rem 2rem;
    font-size: 16px;
    border: 2px solid #aaa;
    background-color: white;
    cursor: pointer;
    border-radius: 6px;
    transition: background-color 0.3s;
    font-family: 'Noto Sans', sans-serif;
    width: 160px;
    text-align: center;
  }
  .tab-button:hover {
    background-color: #eee;
  }
  .tab-button.active {
    background-color: #ddd;
    font-weight: bold;
  }
  .tab-content {
    display: none;
    max-width: 800px;
    margin: 1rem auto;
    text-align: justify;
    font-size: 16px;
  }
  .tab-content.active {
    display: block;
  }
</style>

<div class="tab-container">
  <button class="tab-button active" onclick="showTab('vit')">ViT</button>
  <button class="tab-button" onclick="showTab('beit')">BEiT</button>
  <button class="tab-button" onclick="showTab('swin')">Swin</button>
</div>

<div id="vit" class="tab-content active" style="margin-bottom: 4rem;">
  The Vision Transformer (ViT) applies the standard Transformer architecture directly to image patches, treating them as sequences similar to words in natural language. It splits the input image into fixed-size patches and processes them with self-attention mechanisms. ViT is known for its simplicity and scalability, performing well with large datasets but requiring significant data and compute resources to outperform CNN-based models.
  <div style="text-align: center; margin-top: 1rem;">
    <img src="./Assets/ViT.png" alt="ViT Image" style="max-width: 75%; margin-top: 1rem;">
    <div style="margin-top: 1rem;">
      <a href="./Assets/vit_transformer.ipynb" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333;">ViT Code(Download)</a>
    </div>
  </div>
</div>

<div id="beit" class="tab-content" style="margin-bottom: 4rem;">
  BEiT builds upon ViT by introducing a pretraining strategy similar to BERT in NLP. It treats image patches as discrete visual tokens and learns bidirectional representations using a masked image modeling objective. This enables the model to better capture contextual relationships within the image, significantly improving performance in downstream tasks, especially when labeled data is limited.
  <div style="text-align: center; margin-top: 1rem;">
    <img src="./Assets/BEiT.png" alt="BEiT Image" style="max-width: 75%; margin-top: 1rem;">
    <div style="margin-top: 1rem;">
      <a href="./Assets/BEiT_transformer.ipynb" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333;">BEiT Code(Download)</a>
    </div>
  </div>
</div>

<div id="swin" class="tab-content" style="margin-bottom: 4rem;">
  The Swin Transformer introduces a hierarchical architecture that processes images through non-overlapping local windows with shifted configurations across layers. This design enables both local and global representation learning while maintaining computational efficiency. Its ability to model long-range dependencies and multi-scale features makes it particularly effective for dense prediction tasks such as detection and segmentation.
  <div style="text-align: center; margin-top: 1rem;">
    <img src="./Assets/Swin.png" alt="Swin Image" style="max-width: 75%; margin-top: 1rem;">
    <div style="margin-top: 1rem;">
      <a href="./Assets/swin_transformer.py" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333;">Swin Code(Download)</a>
    </div>
  </div>
</div>

<script>
  function showTab(id) {
    const tabs = document.querySelectorAll('.tab-button');
    const contents = document.querySelectorAll('.tab-content');
    tabs.forEach(tab => tab.classList.remove('active'));
    contents.forEach(content => content.classList.remove('active'));
    document.querySelector(`#${id}`).classList.add('active');
    event.target.classList.add('active');
  }
</script>
  
<div class="figure">
  <img src="./Assets/figure1.png" alt="Figure 1">
  <div class="caption">
<p>
  The graph presents a comparative evaluation of three transformer-based models: 
  <span class="highlight">Vision Transformer (ViT)</span>, 
  <span class="highlight">Swin Transformer</span>, and 
  <span class="highlight">BEiT</span>. Performance is tracked over 10 training epochs using 
  <span class="highlight">accuracy</span> and 
  <span class="highlight">loss</span> metrics on both training and test sets.
</p>

<p>
  From the accuracy graph (left), the 
  <span class="highlight">Swin Transformer</span> shows the highest training accuracy, but it underperforms on the test set. In contrast, the 
  <span class="highlight">BEiT</span> model achieves the best test accuracy across epochs, suggesting stronger generalization to unseen data.
</p>

<p>
  The loss graph (right) supports this observation. 
  <span class="highlight">BEiT</span> maintains the lowest test loss throughout, indicating more consistent and reliable predictions. Meanwhile, 
  <span class="highlight">ViT</span> and 
  <span class="highlight">Swin</span> show either plateauing or increasing test loss, pointing to possible overfitting.
</p>

<p>
  Overall, <span class="highlight">BEiT</span> demonstrates the best balance between learning and generalization, making it the most robust among the three models in this evaluation.
</p>
  </div>
</div>

<div class="figure">
  <img src="./Assets/figure2.png" alt="Figure 2">
  <div class="caption" style="margin-bottom: 3rem;">
    <b>Figure 2 :</b> presents a performance comparison of the Vision Transformer (ViT) model trained on the full CheXpert dataset versus its downsampled counterpart. As illustrated in the graphs, both accuracy and loss metrics remain nearly identical across training epochs. This outcome suggests that the ViT model maintains comparable performance even when trained on a reduced dataset, indicating a degree of robustness. Such results highlight the potential for significant computational efficiency without compromising diagnostic effectiveness.
  </div>
</div>

<div class="figure">
  <img src="./Assets/figure3.1.png" alt="Figure 3">
  <div class="caption">
    <b>Figure 3 :</b> The CLS token, which is trained to aggregate global information from the entire image, can be used to generate 2D heatmaps by visualizing the attention scores assigned to each patch. Figure 3 illustrates how the attention maps evolve as the model improves: the earlier model fails to focus accurately on the lesion, while the updated model correctly identifies disease-relevant regions.
  </div>
</div>

<div class="abstract-title">Discussion And Future Work</div>
<div class="abstract-body">
  We evaluated three transformer (ViT, Swin, and BEiT TransFormer) on the CheXpert dataset, using both full and reduced versions to assess their performance. Among them, BEiT demonstrated the most robust results, attributed to its use of masked-image modeling during pre-training, which enhances generalization and the ability to capture diverse image features. For future work, we plan to conduct robustness tests by introducing small Gaussian noise or perturbations to input images, measuring performance degradation, and applying additional training to improve model resilience if necessary.
</div>

<div class="abstract-title">Final Presentation Q&A</div>
<div class="abstract-body">
  Q1. "How was overfitting in ViT heatmaps handled?"<br>
  A1. <br>
  Q2. “What are the insights into BEiT’s superior performance?”<br>
  A2. 
</div>
  
<div style="text-align: center; margin-top: 4rem;">
  <a href="./Assets/Transformer For Medical AI.pdf" target="_blank" style="text-decoration: none; font-size: 16px; border: 1px solid #888; padding: 0.4rem 1rem; border-radius: 6px; color: #333; margin: 0 0.5rem;">
    Final Presentation Report
  </a>
</div>

</body>
</html>
